#![deny(clippy::all)]

use anyhow::Result;
use clap::Parser;
use llm_client::{LlmConfig, LlmProvider, Prompt, Provider, provider_from_config};
use std::process::Command;
use terminal_core::{Block, run, CommandOutput};
mod context;
use context::ContextEngine;
use tokio::io::{self, AsyncBufReadExt, BufReader};
use tokio_stream::StreamExt;

#[derive(Parser, Debug)]
#[command(author, version, about)]
struct Cli {
    /// LLM provider to use
    #[arg(long, default_value = "ollama")]
    provider: String,
    /// Model name
    #[arg(long, default_value = "llama3")]
    model: String,
    /// Opt into telemetry
    #[arg(long, default_value_t = false)]
    insecure_telemetry: bool,
}

enum Route {
    Spawn(String),
    Switch(String),
    Exec { cmd: String, rationale: String },
}

struct CommandRouter {
    cfg: LlmConfig,
    provider: Box<dyn LlmProvider>,
    context: ContextEngine,
}

impl CommandRouter {
    fn new(cfg: LlmConfig, context: ContextEngine) -> Self {
        let provider = provider_from_config(&cfg);
        Self { cfg, provider, context }
    }

    #[cfg(test)]
    fn with_provider(cfg: LlmConfig, provider: Box<dyn LlmProvider>, context: ContextEngine) -> Self {
        Self { cfg, provider, context }
    }

    async fn nl_to_shell(&self, line: &str) -> Result<(String, String)> {
        if let Some(cmd) = self.context.cached_cmd(line) {
            return Ok((cmd, "cached".into()));
        }
        let input = format!("{}\n{}", self.context.context(), line);
        let resp = self
            .provider
            .complete(Prompt {
                text: input,
            })
            .await?;
        Ok((resp.text, "generated by ai".into()))
    }

    async fn route(&mut self, line: &str) -> Result<Route> {
        let trimmed = line.trim();
        if trimmed == "bash" || trimmed == "pwsh" || trimmed == "cmd" {
            return Ok(Route::Spawn(trimmed.into()));
        }
        if let Some(rest) = trimmed.strip_prefix("/model ") {
            self.cfg.model = rest.to_string();
            self.provider = provider_from_config(&self.cfg);
            return Ok(Route::Switch(rest.to_string()));
        }
        let (cmd, rationale) = self.nl_to_shell(trimmed).await?;
        Ok(Route::Exec { cmd, rationale })
    }

    async fn handle_line(&mut self, line: &str) -> Result<()> {
        match self.route(line).await? {
            Route::Spawn(shell) => {
                let mut cmd = Command::new(shell);
                let CommandOutput { mut blocks, exit } = run(cmd).await?;
                while let Some(Block { text }) = blocks.next().await {
                    println!("{}", text);
                    self.context.push(Block { text });
                }
                let code = exit.await.unwrap_or(1);
                self.context.push(Block { text: format!("exit: {code}") });
                if code != 0 {
                    println!("Fix?");
                }
            }
            Route::Switch(model) => {
                println!("Switched model to {model}");
            }
            Route::Exec { cmd, rationale } => {
                println!("# AI: {rationale}");
                let mut command = if cfg!(target_os = "windows") {
                    let mut c = Command::new("cmd");
                    c.arg("/C").arg(&cmd);
                    c
                } else {
                    let mut c = Command::new("sh");
                    c.arg("-c").arg(&cmd);
                    c
                };
                let CommandOutput { mut blocks, exit } = run(command).await?;
                while let Some(Block { text }) = blocks.next().await {
                    println!("{}", text);
                    self.context.push(Block { text });
                }
                let code = exit.await.unwrap_or(1);
                if code == 0 {
                    self.context.cache_translation(line, &cmd);
                }
                self.context.push(Block { text: format!("exit: {code}") });
                if code != 0 {
                    println!("Fix?");
                }
            }
        }
        Ok(())
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Cli::parse();
    let cfg = LlmConfig {
        provider: match args.provider.as_str() {
            "ollama" => Provider::Ollama,
            "openrouter" => Provider::OpenRouter,
            "aifoundry" => Provider::AIFoundry,
            _ => Provider::Custom,
        },
        base_url: "http://localhost".into(),
        api_key: None,
        model: args.model.clone(),
    };

    if args.insecure_telemetry {
        println!("Telemetry enabled");
    }

    let context = ContextEngine::new("context.db");
    let mut router = CommandRouter::new(cfg, context);
    let stdin = BufReader::new(tokio::io::stdin());
    let mut lines = stdin.lines();
    while let Some(line) = lines.next_line().await? {
        router.handle_line(&line).await?;
    }
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    use rstest::rstest;

    #[derive(Clone)]
    struct FakeProvider;

    use async_trait::async_trait;

    #[async_trait]
    impl LlmProvider for FakeProvider {
        async fn complete(&self, req: Prompt) -> Result<llm_client::Resp> {
            Ok(llm_client::Resp {
                text: format!("echo {}", req.text),
            })
        }
    }

    #[rstest]
    #[case("--provider", "ollama", Provider::Ollama)]
    #[case("--provider", "openrouter", Provider::OpenRouter)]
    fn parse_provider(#[case] flag: &str, #[case] value: &str, #[case] expected: Provider) {
        let cli = Cli::try_parse_from(["test", flag, value]).unwrap();
        let provider = match cli.provider.as_str() {
            "ollama" => Provider::Ollama,
            "openrouter" => Provider::OpenRouter,
            "aifoundry" => Provider::AIFoundry,
            _ => Provider::Custom,
        };
        match (&provider, &expected) {
            (Provider::Ollama, Provider::Ollama) | (Provider::OpenRouter, Provider::OpenRouter) => {
            }
            _ => panic!("mismatch"),
        }
    }

    #[rstest]
    #[tokio::test]
    async fn route_spawn() {
        let cfg = LlmConfig {
            provider: Provider::Ollama,
            base_url: "".into(),
            api_key: None,
            model: "m".into(),
        };
        let dir = tempfile::tempdir().unwrap();
        let ctx = ContextEngine::new(dir.path().to_str().unwrap());
        let mut router = CommandRouter::with_provider(cfg, Box::new(FakeProvider), ctx);
        match router.route("bash").await.unwrap() {
            Route::Spawn(s) => assert_eq!(s, "bash"),
            _ => panic!(),
        }
    }

    #[rstest]
    #[tokio::test]
    async fn route_switch() {
        let cfg = LlmConfig {
            provider: Provider::Ollama,
            base_url: "".into(),
            api_key: None,
            model: "m".into(),
        };
        let dir = tempfile::tempdir().unwrap();
        let ctx = ContextEngine::new(dir.path().to_str().unwrap());
        let mut router = CommandRouter::with_provider(cfg, Box::new(FakeProvider), ctx);
        match router.route("/model x").await.unwrap() {
            Route::Switch(m) => assert_eq!(m, "x"),
            _ => panic!(),
        }
    }

    #[rstest]
    #[tokio::test]
    async fn route_exec() {
        let cfg = LlmConfig {
            provider: Provider::Ollama,
            base_url: "".into(),
            api_key: None,
            model: "m".into(),
        };
        let dir = tempfile::tempdir().unwrap();
        let ctx = ContextEngine::new(dir.path().to_str().unwrap());
        let mut router = CommandRouter::with_provider(cfg, Box::new(FakeProvider), ctx);
        match router.route("list files").await.unwrap() {
            Route::Exec { cmd, rationale: _ } => assert_eq!(cmd, "echo list files"),
            _ => panic!(),
        }
    }
}
